\documentclass[a4paper, 12pt]{article}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{listings}
\usepackage{hyperref}

\lstset{language=python}


\title{m$^2$l$^2$: My Machine Learning Library}
\author{Hauke Neitzel}

\begin{document}
\maketitle
\newpage

\section{Introduction}
\subsection{About this library}
"m$^2$l$^2$" is a small machine learning library in Python I have written / am writing during my summer 2015 internship at the Cavendish Laboratory, Lab for Scientific Computing (LabSC). The primary purpose of the library is not to be much use for actual machine learning, instead it is mostly an educational tool for me and possibly others. There exist mature machine learning libraries for many languages (e.g. \texttt{scikit-learn} for python) and I have neither the expertise nor the time to implement the algorithms in a complete (all the different options and statistical considerations) and efficient (well-optimised) manner.

\subsection{Algorithm sources}
The algorithms as implemented are primarily based on lecture slide provided to me by my supervisor Dr. Anita Faul, with a few algorithms coming from \textit{Pattern Recognition and Machine Learning} (\cite{PRML}), which was also used for occasional clarification and a lot of statistical background.


\section{Structure}
The library is divided into several modules corresponding to different general areas in machine learning. This section will give an overview of how the algorithms in the different modules are used, while section 3 contains documentation for each individual algorithm.

For some of the iterative algorithms, the class will contain a method called \texttt{getIterationData(i)} which will return the relevant data from the \texttt{i}th iteration. The return type varies with the algorithm, so this is not an interface where different algorithms can just be swapped out in general (there may be groups of algorithms with similar intermediate data).

\subsection{\texttt{m2l2.classification}}
The algorithms in this module deal with the task of classification, i.e. deciding what class $\mathcal{C}_k, k \in \{0 \ldots K-1\}$ a set of features (feature vector $\mathbf{x} = \{x_0, x_1, \ldots, x_n\}$) belongs to based on a set of training data $\{(\mathbf{x}_i, c_i)\}, c_i \in \{\mathcal{C}_k\}$.

The algorithms are implemented as classes to retain the parameters and training data given. After a classifier is created (in some cases taking parameters for the algorithm), it can be trained on some training data. Once it is trained it can be used to classify further feature vectors.

\begin{lstlisting}[frame=TLbr,breaklines=true]
class Classifier:
  def __init__(self, parameters):
    # `parameters' can be different things used to
    # specify the exact operation of the classifier.
    # This includes the kernel for a SVM and the
    # number of classes for multi-class classifiers.
  
  def train(self, X, y):
    # X is an array of feature vectors and y is an
    # array of the associated classes. The classes
    # are expected to be numbered 0 to K-1
  
  def classify(self, x):
    # x is a feature vector to be classified.
    # For a binary classifier, a negative result means
    # class 0, a positive result class 1, with the
    # absolute value giving some measure of certainty.
    # For multi-class classifiers, the result is just
    # the number of the class.
\end{lstlisting}

\subsection{\texttt{m2l2.clustering}}
Clustering is a form of unsupervised learning where the aim is to find groups of points that lie close together and separated from others in data-space. The probabilistic models on which the algorithms in this module are based are discrete latent-variable models
\[ p(\mathbf{x}) = \sum_\mathbf{z} p(\mathbf{z}) p(\mathbf{x} | \mathbf{z}) \]
where $\mathbf{z}$ is the discrete latent variable.

The algorithms in this module are implemented as classes that allow some greater insight into the algorithms operation. Since most algorithms in this module use some variant of the iterative EM-algorithm, those classes provide a way to either single step through the iterations or to run the algorithm to convergence.

\begin{lstlisting}[frame=TLbr,breaklines=true]
class Clustering:
  def __init__(self, X, parameters):
    # `parameters' can be different things, usually
    # any model parameters that have to be provided
    # in advance, such as the number of clusters
    # expected in the data
  
  def run(self, parameters):
    # Run the algorithm to convergence, `parameters'
    # can be convergence threshold values for different
    # parameters (usually cost or likelihood function)
  
  def E_Step(self):
  def M_Step(self):
    # For algorithms based on EM, these functions
    # perform a single step of the iteration
    
  def getCurrentParameters(self):
    # returns the parameters of the model as currently
    # determined by the algorithm. This includes the
    # cluster assignments of the data-points and the
    # parameters (mean etc.) of the clusters
\end{lstlisting}

\subsection{\texttt{m2l2.utils}}
This module contains a few routines for data preprocessing and dimensionality reduction. The routines are implemented as simple functions that return the processed dataset.

\begin{lstlisting}[frame=TLbr,breaklines=true]
def process_data(X, options, return_parameters=False):
  # `options' are things like number of desired
  # dimensions for dimensionality reduction.
  # `return_parameters' controls whether the function
  # returns the parameters (like the data mean or the
  # basis vector of the subspace) that were found and
  # used to transform the data in addition to the
  # transformed data.
\end{lstlisting}


\section{Documentation}
This section contains some information on each of the algorithms implemented in this library. Usually the information is mostly on my implementation, motivation for them and derivation should be found in the relevant textbook or lecture notes of your choice.

\subsection{\texttt{m2l2.classification}}
\subsubsection{Gaussian Naive Bayes' (as class \texttt{NaiveBayes()})}
The Gaussian Naive Bayes' classifier assumes that the data comes from multivariate Gaussian distributions with mean $\bm{\mu}$ and covariance matrix $\bm{\Sigma}$ which we estimate from the training data. This gives a likelihood $p(\mathbf{x} | \mathcal{C}_k) = \mathcal{N}(\mathbf{x} | \bm{\mu}_k, \bm{\Sigma}_k) = \frac{1}{\sqrt{2 \pi |\bm{\Sigma}_k|}} \exp\left(\frac{1}{2} (\mathbf{x} - \bm{\mu}_k)^T\bm{\Sigma}_k^{-1}(\mathbf{x} - \bm{\mu}_k)\right)$ and using the number of training points for each class as the prior probability we get the posterior probability $p(\mathcal{C}_k | \mathbf{x}) \propto \mathcal{N}(\mathbf{x} | \bm{\mu}_k, \bm{\Sigma}_k) p(\mathcal{C}_k)$ from Bayes' Theorem. A point is then classified by which class gives the maximum posterior probability.

The implementation can be used for both binary and multi-class classification, the number of classes used is determined by the keyword argument \texttt{K} of the constructor (default is 2).
    
\subsubsection{Discriminant Analysis (as class \texttt{DA()})}
Discriminant Analysis is implemented as a binary classifier in both the linear and the quadratic variant. A point $\mathbf{x}$ is classified according to the sign of $\mathbf{x}^T\mathbf{Ax} + \mathbf{bx} + c$ where
\begin{align*}
\mathbf{A} &= \frac{1}{2} \left(\bm{\Sigma}_1^{-1} - \bm{\Sigma}_0^{-1}\right) \\
\mathbf{b} &= \bm{\Sigma}_0^{-1}\bm{\mu}_0 - \bm{\Sigma}_1^{-1}\bm{\mu}_1 \\
c &= \frac{1}{2} \left( \log\frac{|\bm{\Sigma}_1|}{|\bm{\Sigma}_0|} + \bm{\mu}_1^T\bm{\Sigma}_1^{-1}\bm{\mu}_1 - \bm{\mu}_0^T\bm{\Sigma}_0^{-1}\bm{\mu}_0 \right) \\
\end{align*}
and $\bm{\mu}_0$, $\bm{\mu}_1$, $\bm{\Sigma}_0$ and $\bm{\Sigma}_1$ are estimated from the training data and in the linear case, we assume $\bm{\Sigma}_0 = \bm{\Sigma}_1 \equiv \bm{\Sigma}$, which makes $\mathbf{A}$ vanish.

The constructor takes a keyword argument \texttt{datype} which can be set to \texttt{`quadratic'} to use QDA, otherwise LDA is used.

\subsubsection{k-Nearest-Neighbours (as class \texttt{kNN()})}
The k-Nearest-Neighbours algorithm is implemented with a kd-tree from the \texttt{scipy} library, since classification would otherwise have complexity $O(n + k \log n)$ per point where $n$ is the number of training points. The kd-tree manages to reduce this complexity to $O(k \log n)$, however it gets less effective if the number of dimensions is large.

The number of nearest neighbours used is determined by the keyword argument \texttt{k} of the constructor (default is 3).

\subsubsection{Support Vector Machine (as class \texttt{SVM()})}
The implementation of the SVM can use different kernels which can be set with the \texttt{kernel} keyword argument of the constructor. The default is a linear kernel, the argument \texttt{`rbf'} can be used to specify that the inbuilt gaussian radial basis function should be used and \texttt{`custom'} allows any kernel to be specified. If the rbf is used, the keyword argument \texttt{sigma} can be used to specify its width. The custom kernel function needs to be supplied via the \texttt{customK} argument and should be a function of two arguments, which should be able to handle being given a list of points for either argument to operate in a vectorised manner (actually, additionally it needs to accept \texttt{self} as its first argument).

\subsubsection{Adaptive Boosting (as class \texttt{AdaBoost()})}
The implementation of AdaBoost is as detailed in \cite{PRML} using one-level decision trees as weak learners (in the library as \texttt{DecisionBranch()}). The constructor takes as an argument the number of weak learners to use (which should not be chosen too high to avoid overfitting, but should be large enough to capture the structure of the data).

A different weak learner can also be specified with the \texttt{baseClassifier} argument. This classifier should have the same structure as the others in this module, except that it must be able to train on a weighted data set, i.e. it should place more importance on the samples with higher weights. The weight is given as a third argument to the \texttt{train()} function.

\subsubsection{One vs. All and One vs. One (as classes \texttt{OVA()} and \texttt{OVO()})}
These two are a sort of meta-classifier, using any binary classifier to create a multi-class classifier. The number of classes and the base classifier are passed to the constructor in both cases. They differ in the approach and the total number of classifiers used. The One vs. All (also called One vs. Rest or One vs. Many) uses $K$ classifiers, where the One vs. One classifier uses $\frac{K(K-1)}{2} = O(K^2)$ classifiers.

\subsection{\texttt{m2l2.clustering}}
\subsubsection{K-Means (as class \texttt{kMeans})}

\subsubsection{Gaussian Mixture Model (as class \texttt{GaussianMixtureEM}}

\subsection{\texttt{m2l2.utils}}
\subsubsection{Data normalisation (as function \texttt{normalise})}

\subsubsection{Principle Component Analysis (as function \texttt{PCA})}

\subsubsection{Linear Discriminant Analysis (as function \texttt{LDA})}

\begin{thebibliography}{9}
\bibitem{PRML} C.M. Bishop, \emph{Pattern Recognition and Machine Learning}, Springer, 2007
\end{thebibliography}

\end{document}