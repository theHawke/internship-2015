\documentclass[a4paper]{article}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{listings}
\usepackage{hyperref}

\lstset{language=python}

\title{A Survey of Machine Learning Methods and Example Implementations thereof}
\author{Hauke Neitzel (hn276)}
\date{Long Vacation Project 2015}

\begin{document}
\maketitle

\abstract{
Machine learning is an immensely popular field at the moment, with a lot of interest in it coming from real world applications like data mining. However, machine learning has also found applications in Physics where it is used primarily in situations where there is so much data that it would be infeasible for humans to look at it.

With this in mind, the aim of this project was to gain a thorough familiarity with many of the machine learning algorithms commonly in use today, with the supplementary goal of producing a library of example implementations of those algorithms. In this report I present the algorithms included in the library and include general documentation of the library's structure. The library is written in python, a popular language for scientific computing, utilising the numpy and scipy libraries.
}

\section{Introduction}
Machine learning as a field of independent academic study became popular in the early 1990s, developing out of the field of Artificial Intelligence, where research primarily went in different directions. Many of the algorithms used in machine learning were developed well before, but in the 1990s attention shifted to a statistical approach to learning, which led to the development of many new models. Also, the increase in available computing power enabled the use of many techniques that were previously infeasible, such as Monte-Carlo methods for models with analytically intractable parts. A last contributing factor to its popularity today is the increased availability of data through the internet, on which machine learning is used for data mining.

In Physics, machine learning has also found several applications, the most prominent of which is probably the processing of large data sets. In many areas of Physics, there are large experiments nowadays which generate large amounts of data which are infeasible for humans to look at. Learning algorithms are used to extract the interesting data that scientists work with to infer information about their models. Another use of machine learning in physics that is currently being developed is the use of learning algorithms to model complex systems. These are often many-body systems or materials which can't be modelled analytically and are too large for direct computational simulation, however relationships between different properties of the system can often be inferred by learning algorithms.

%TODO

The algorithms as implemented are primarily based on lecture slide provided to me by my supervisor Dr. Anita Faul, and \textit{Pattern Recognition and Machine Learning} (\cite{PRML}). For a few of the more complicated algorithms, I consulted the primary literature to gain more insight into the model used (as indicated in the respective descriptions).

This report has two primary sections. Section 2 gives a description of all the algorithms that I researched and implemented in the library, while Section 3 explains the structure of the different parts of the library.


\section{Algorithms}
This section contains some information on each of the algorithms implemented in this library. %TODO

\subsection{Classification}
Classification is a type of supervised learning, i.e. we are gives a set of training data for which we already know a quantity of interest and we want to make predictions about that quantity for future data. For classification, the quantity of interest is the class that a data-point is assigned to, i.e. a discrete label that partitions the data into multiple subsets.

\subsubsection{Gaussian Naive Bayes'}
The Gaussian Naive Bayes' classifier assumes that the data in each class comes from separate multivariate Gaussian distributions with mean $\bm{\mu}_k$ and covariance $\bm{\Sigma}_k$. The assumption made by a Naive Bayes' classifier is that the different features are conditionally independent given the class, which allows for faster calculation of the results since the covariance matrix is diagonal. With $\bm{\mu}_k$ and $\bm{\Sigma}_k$ estimated form the data, we can calculate the likelihood that a datum belongs to a specific class as

\[ p(\mathbf{x} | \mathcal{C}_k) = \mathcal{N}(\mathbf{x} | \bm{\mu}_k, \bm{\Sigma}_k) = \frac{1}{\sqrt{2 \pi |\bm{\Sigma}_k|}} \exp\left(\frac{1}{2} (\mathbf{x} - \bm{\mu}_k)^T\bm{\Sigma}_k^{-1}(\mathbf{x} - \bm{\mu}_k)\right) \] 

Then, using Bayes' Theorem and the number of training points for each class $n_k$ as the prior probability we get the posterior probability for the class

\[ p(\mathcal{C}_k | \mathbf{x}) \propto p(\mathbf{x} | \mathcal{C}_k) p(\mathcal{C}_k) = n_k \mathcal{N}(\mathbf{x} | \bm{\mu}_k, \bm{\Sigma}_k)\]

A point is then classified by which class gives the maximum posterior probability (often called the maximum a-posteriori value $\mathcal{C}_\textsc{map}$).

Since $\mathcal{C}_\textsc{map}$ is used to determine the class, Naive Bayes' can be used with any number of classes. The implementation automatically detects how many classes are in the model and in the case of two classes will return the log of the ratios of the posteriors while for multiple classes it will return the class label.
    
\subsubsection{Discriminant Analysis}
Discriminant analysis has a similar starting point to the Naive Bayes' classifier. It only works on two classes and assumes that they are normal-distributed with mean $\bm{\mu}_k$ and covariance $\bm{\Sigma}_k$ where $k = 0,1$ respectively which are obtained from the training data. Unlike Naive Bayes', it does not make the assumption of conditional independence, however, the Linear Discriminant Analysis (LDA) variant makes the assumption that both classes are distributed with the same covariance (in practice we use the average of the two covariances). Looking at the likelihood ratio, we get
\[ \frac{p(\mathbf{x} | \mathcal{C}_1)}{p(\mathbf{x} | \mathcal{C}_0)} = \frac{|\bm{\Sigma}_0|}{|\bm{\Sigma}_1|} \exp \left\lbrace \frac{1}{2} \left( (\mathbf{x} - \bm{\mu}_0)^T \bm{\Sigma}_0^{-1} (\mathbf{x} - \bm{\mu}_0) - (\mathbf{x} - \bm{\mu}_1)^T \bm{\Sigma}_1^{-1} (\mathbf{x} - \bm{\mu}_1) \right) \right\rbrace \]
Taking the log of this quantity and rearranging slightly, we can classify a point $\mathbf{x}$ according to the sign of $\mathbf{x}^T\mathbf{Ax} + \mathbf{bx} + c$ where
\begin{align*}
\mathbf{A} &= \frac{1}{2} \left(\bm{\Sigma}_0^{-1} - \bm{\Sigma}_1^{-1}\right) \\
\mathbf{b} &= \bm{\Sigma}_1^{-1}\bm{\mu}_1 - \bm{\Sigma}_0^{-1}\bm{\mu}_0 \\
c &= \frac{1}{2} \left( \log\frac{|\bm{\Sigma}_0|}{|\bm{\Sigma}_1|} + \bm{\mu}_0^T\bm{\Sigma}_0^{-1}\bm{\mu}_0 - \bm{\mu}_1^T\bm{\Sigma}_1^{-1}\bm{\mu}_1 \right) \\
\end{align*}
For LDA, $A$ and the first term in $c$ will vanish since the covariances are equal, leaving only $ \mathbf{bx} + c$, whereas for Quadratic Discriminant Analysis (QDA) the full expression is used.

The implementation takes a keyword argument \texttt{datype} in its constructor which can be set to \texttt{`quadratic'} to use QDA, otherwise LDA is used.

\subsubsection{k-Nearest-Neighbours}
The k-Nearest-Neighbours (kNN) algorithm is a conceptually very simple algorithm that classifies points according to the most common class among the k closest points from the training set.

The biggest problem with it is that the entire training set has to be considered for each point that is classified. A naive implementation that computes the distance to every training point has complexity $O(n + k \log k)$ using partial sorting. More sophisticated algorithms use data structures aware of the space in which the points lie (often some form of metric tree) which are constructed at training time, but reduce the classification complexity to something like $O(k \log n)$. kNN also suffers strongly from the curse of dimensionality, as many of the data structures become inefficient at high dimensions and also, many points will have similar distances, making the lookup less reliable.

The implementation uses kd-trees from the \texttt{scipy} library to achieve the $O(k \log n)$ complexity (in low dimensions) and the number $k$ of nearest neighbours used is determined by the keyword argument \texttt{k} of the constructor (default is 1).

\subsubsection{Support Vector Machine} %TODO
The implementation of the SVM can use different kernels which can be set with the \texttt{kernel} keyword argument of the constructor. The default is a linear kernel, the argument \texttt{`rbf'} can be used to specify that the inbuilt gaussian radial basis function should be used and \texttt{`custom'} allows any kernel to be specified. If the rbf is used, the keyword argument \texttt{sigma} can be used to specify its width. The custom kernel function needs to be supplied via the \texttt{customK} argument and should be a function of two arguments, which should be able to handle being given a list of points for either argument to operate in a vectorised manner (actually, additionally it needs to accept \texttt{self} as its first argument).

\subsubsection{Adaptive Boosting}
The implementation of AdaBoost is as detailed in \cite{PRML} using one-level decision trees as weak learners (in the library as \texttt{DecisionBranch()}). The constructor takes as an argument the number of weak learners to use (which should not be chosen too high to avoid overfitting, but should be large enough to capture the structure of the data).

A different weak learner can also be specified with the \texttt{baseClassifier} argument. This classifier should have the same structure as the others in this module, except that it must be able to train on a weighted data set, i.e. it should place more importance on the samples with higher weights. The weight is given as a third argument to the \texttt{train()} function.

\subsubsection{Multi-Class Meta Classifiers}
Since many classifiers are by design restricted to two classes to take advantage of certain mathematical properties (e.g. Discriminant Analysis, SVM), the one-vs-all (OVA) and one-vs-one (OVO) strategies are a way of combining several binary classifiers to construct a mult-class classifier.

The OVA strategy uses $k$ classifiers for $k$-class classification, each of which uses the points in one class against all the other points. It selects the class which gets the highest score against all the others. A problem with this approach is that the classifiers get skewed distributions (one small class and one very large class) which can be a problem depending on the underlying classifier used (example: see figure %TODO OVA LDA figure% ).

The OVO strategy uses $\frac{k(k-1)}{2}$ classifiers, one for every pair of classes. The class is determined by majority vote of the classifiers which can lead to ties between classes, but those can usually be resolved by looking at the direct comparison between the tied classes. While it has the advantage against OVA of giving even distributions to the underlying classifiers, it is computationally more expensive as the number of classes grows.


\subsection{Regression}


\subsection{Clustering}
\subsubsection{K-Means}

\subsubsection{Gaussian Mixture Model}

\subsubsection{Dirichlet Process Mixture Model}


\subsection{Other Algorithms}
\subsubsection{Data normalisation}

\subsubsection{Principle Component Analysis}

\subsubsection{Linear Discriminant Analysis}


\section{Library Structure}
The library is divided into several modules corresponding to different general areas in machine learning. This section will give an overview of how the algorithms in the different modules are used, while section 3 contains documentation for each individual algorithm.

For some of the iterative algorithms, the class will contain a method called \texttt{getIterationData(i)} which will return the relevant data from the \texttt{i}th iteration. The return type varies with the algorithm, so this is not an interface where different algorithms can just be swapped out in general (there may be groups of algorithms with similar intermediate data).

\subsection{\texttt{m2l2.classification}}
The algorithms in this module deal with the task of classification, i.e. deciding what class $\mathcal{C}_k, k \in \{0 \ldots K-1\}$ a set of features (feature vector $\mathbf{x} = \{x_0, x_1, \ldots, x_n\}$) belongs to based on a set of training data $\{(\mathbf{x}_i, c_i)\}, c_i \in \{\mathcal{C}_k\}$.

The algorithms are implemented as classes to retain the parameters and training data given. After a classifier is created (in some cases taking parameters for the algorithm), it can be trained on some training data. Once it is trained it can be used to classify further feature vectors.

\begin{lstlisting}[frame=TLbr,breaklines=true]
class Classifier:
  def __init__(self, parameters):
    # `parameters' can be different things used to
    # specify the exact operation of the classifier.
    # This includes the kernel for a SVM and the
    # number of classes for multi-class classifiers.
  
  def train(self, X, y):
    # X is an array of feature vectors and y is an
    # array of the associated classes. The classes
    # are expected to be numbered 0 to K-1
  
  def classify(self, x):
    # x is a feature vector to be classified.
    # For a binary classifier, a negative result means
    # class 0, a positive result class 1, with the
    # absolute value giving some measure of certainty.
    # For multi-class classifiers, the result is just
    # the number of the class.
\end{lstlisting}

\subsection{\texttt{m2l2.regression}}

\subsection{\texttt{m2l2.clustering}}
Clustering is a form of unsupervised learning where the aim is to find groups of points that lie close together and separated from others in data-space. The probabilistic models on which the algorithms in this module are based are discrete latent-variable models
\[ p(\mathbf{x}) = \sum_\mathbf{z} p(\mathbf{z}) p(\mathbf{x} | \mathbf{z}) \]
where $\mathbf{z}$ is the discrete latent variable.

The algorithms in this module are implemented as classes that allow some greater insight into the algorithms operation. Since most algorithms in this module use some variant of the iterative EM-algorithm, those classes provide a way to either single step through the iterations or to run the algorithm to convergence.

\begin{lstlisting}[frame=TLbr,breaklines=true]
class Clustering:
  def __init__(self, X, parameters):
    # `parameters' can be different things, usually
    # any model parameters that have to be provided
    # in advance, such as the number of clusters
    # expected in the data
  
  def run(self, parameters):
    # Run the algorithm to convergence, `parameters'
    # can be convergence threshold values for different
    # parameters (usually cost or likelihood function)
  
  def E_Step(self):
  def M_Step(self):
    # For algorithms based on EM, these functions
    # perform a single step of the iteration
    
  def getCurrentParameters(self):
    # returns the parameters of the model as currently
    # determined by the algorithm. This includes the
    # cluster assignments of the data-points and the
    # parameters (mean etc.) of the clusters
\end{lstlisting}

\subsection{\texttt{m2l2.misc}}
This module contains a few routines for data preprocessing and dimensionality reduction. The routines are implemented as simple functions that return the processed dataset.

\begin{lstlisting}[frame=TLbr,breaklines=true]
def process_data(X, options, return_parameters=False):
  # `options' are things like number of desired
  # dimensions for dimensionality reduction.
  # `return_parameters' controls whether the function
  # returns the parameters (like the data mean or the
  # basis vector of the subspace) that were found and
  # used to transform the data in addition to the
  # transformed data.
\end{lstlisting}


\section{Conclusion}


\begin{thebibliography}{9}
\bibitem{PRML} C.M. Bishop, \emph{Pattern Recognition and Machine Learning}, Springer, 2007

\end{thebibliography}

\end{document}